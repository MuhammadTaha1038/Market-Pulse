# =============================================================================
# DATA SOURCE CONFIGURATION
# =============================================================================

# Data Source Type
# Options: "excel" (default) or "oracle"
# Use "excel" for manual Excel file uploads
# Use "oracle" to fetch data from Oracle database
DATA_SOURCE=excel

# Excel Input Configuration (when DATA_SOURCE=excel)
EXCEL_INPUT_FILE=Color today.xlsx

# =============================================================================
# ORACLE DATABASE CONFIGURATION (when DATA_SOURCE=oracle)
# =============================================================================

# Oracle Credentials API
# Client's API endpoint that returns credentials in JSON format
# Expected response keys: "eval.jdbc.user" and "eval.jdbc.password"
ORACLE_CREDENTIALS_API_URL=

# Oracle Connection Details
ORACLE_HOST=
ORACLE_PORT=1521
ORACLE_SERVICE_NAME=
ORACLE_USERNAME=
ORACLE_PASSWORD=
ORACLE_TABLE_NAME=COLOR_DATA

# =============================================================================
# OUTPUT DESTINATION CONFIGURATION
# =============================================================================

# Output Destination Type
# Options: "local" (default), "s3", or "both"
# "local" - Save to local Excel file
# "s3" - Upload to AWS S3 bucket
# "both" - Save locally AND upload to S3
OUTPUT_DESTINATION=local

# Local Excel Output Configuration (when OUTPUT_DESTINATION includes "local")
OUTPUT_DIR=
# Leave empty to use project root directory

# =============================================================================
# AWS S3 CONFIGURATION (when OUTPUT_DESTINATION includes "s3")
# =============================================================================

# S3 Bucket Configuration
S3_BUCKET_NAME=
S3_REGION=us-east-1
S3_PREFIX=processed_colors/
# S3_PREFIX is the folder structure inside the bucket (e.g., "processed_colors/")

# AWS Credentials (optional if using IAM role)
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# File Format for S3 Upload
# Options: "xlsx" (default), "csv", "parquet"
S3_FILE_FORMAT=xlsx

# =============================================================================
# BACKEND SERVER CONFIGURATION
# =============================================================================

# API Server Port
PORT=3334

# Environment
ENVIRONMENT=development

# =============================================================================
# CRON JOB CONFIGURATION
# =============================================================================

# Enable/Disable Automated Processing
ENABLE_CRON_JOBS=true

# Cron Schedule (default: every 2 hours between 8 AM - 6 PM)
CRON_SCHEDULE=0 8,10,12,14,16,18 * * *

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Log Level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log File Path
LOG_FILE_PATH=logs/app.log

# =============================================================================
# ADMIN CONFIGURATION
# =============================================================================

# Admin credentials for backend API
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin123

# =============================================================================
# INTEGRATION SETUP INSTRUCTIONS
# =============================================================================

# ORACLE SETUP:
# 1. Get credentials API endpoint from client
# 2. Set ORACLE_CREDENTIALS_API_URL
# 3. Set ORACLE_HOST, ORACLE_PORT, ORACLE_SERVICE_NAME
# 4. Set ORACLE_TABLE_NAME (table/view containing raw color data)
# 5. Change DATA_SOURCE=oracle
# 6. Install: pip install oracledb requests
# 7. Restart backend server

# S3 SETUP:
# 1. Get S3 bucket name and region from client
# 2. Set S3_BUCKET_NAME and S3_REGION
# 3. Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (or use IAM role)
# 4. Set S3_PREFIX for folder structure (optional)
# 5. Choose S3_FILE_FORMAT (xlsx, csv, or parquet)
# 6. Change OUTPUT_DESTINATION=s3 or OUTPUT_DESTINATION=both
# 7. Install: pip install boto3
# 8. Restart backend server

# =============================================================================
# TESTING CONNECTION
# =============================================================================

# After configuration, test connections using:
# Backend API: GET /api/admin/system-status
# This endpoint shows:
# - Data source connection status
# - Output destination status
# - Configuration details
